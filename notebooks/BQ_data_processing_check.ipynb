{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "126c708c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from google.cloud import bigquery\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e3f0d64",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_22001/3402223984.py:53: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_filtered['ds'] = pd.to_datetime(df_filtered['ds'])\n",
      "/tmp/ipykernel_22001/3402223984.py:55: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_filtered['ds'] = df_filtered['ds'].dt.normalize()\n",
      "/tmp/ipykernel_22001/3402223984.py:57: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_filtered['ds'] = pd.to_datetime(df_filtered['ds']).dt.date\n",
      "/tmp/ipykernel_22001/3402223984.py:58: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_filtered[\"ds\"] = pd.to_datetime(df_filtered[\"ds\"])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 850 entries, 0 to 849\n",
      "Data columns (total 2 columns):\n",
      " #   Column   Non-Null Count  Dtype         \n",
      "---  ------   --------------  -----         \n",
      " 0   ds       850 non-null    datetime64[ns]\n",
      " 1   Holiday  850 non-null    Int64         \n",
      "dtypes: Int64(1), datetime64[ns](1)\n",
      "memory usage: 14.2 KB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_22001/3402223984.py:109: UserWarning: Parsing dates in %d/%m/%Y format when dayfirst=False (the default) was specified. Pass `dayfirst=True` or specify a format to silence this warning.\n",
      "  df_unemp_ber['Date'] = pd.to_datetime(df_unemp_ber['Date'])\n"
     ]
    }
   ],
   "source": [
    "#import revenue data with BQ\n",
    "# Initialize a BigQuery client\n",
    "client = bigquery.Client()\n",
    "gcp_project = os.environ['GCP_PROJECT']\n",
    "bq_dataset = os.environ['BQ_DATASET']\n",
    "\n",
    "# full orders BQ query and df\n",
    "query = f\"\"\"\n",
    "SELECT * FROM `{gcp_project}.{bq_dataset}.orders_full`\n",
    "\"\"\"\n",
    "df = client.query(query).to_dataframe()\n",
    "\n",
    "#Dropping unnecessary columns, grouping by \"date\", summing \"item_price\" to get daily revenues\n",
    "df = pd.DataFrame(df.groupby(by=\"date\")[\"item_price\"].sum()/100)\n",
    "df = df.rename(columns={\"date\": \"ds\", \"item_price\": \"y\"})\n",
    "df[\"ds\"] = df.index\n",
    "df = df.reset_index(drop=True)\n",
    "df = df[[\"ds\",\"y\"]]\n",
    "#turning the ds (date) column into datetim\n",
    "df['ds']=pd.to_datetime(df['ds'])\n",
    "#Dropping outliers\n",
    "df = df[df[\"y\"]>=60]\n",
    "df = df[df[\"y\"]<=2300]\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "#remove the duplicates\n",
    "#DELETE duplicates = df['ds'].duplicated()\n",
    "\n",
    "# weather BQ query and df\n",
    "query = f\"\"\"\n",
    "SELECT * FROM `{gcp_project}.{bq_dataset}.weather`\n",
    "\"\"\"\n",
    "weather_df = client.query(query).to_dataframe()\n",
    "\n",
    "#drop some columns\n",
    "weather_df = weather_df.drop(columns=[\"dt\",\"timezone\",\"city_name\",\"lat\",\"lon\",\"sea_level\",\"grnd_level\",\"weather_icon\",\"rain_3h\",\"snow_3h\"])\n",
    "#rename\n",
    "weather_df = weather_df.rename(columns={\"dt_iso\":\"ds\",\"rain_1h\":\"rain\",\"clouds_all\":\"clouds\"})\n",
    "# convert the timestamp column to a datetime object with timezone information\n",
    "weather_df['ds'] = weather_df['ds'].str[:19]\n",
    "weather_df[\"ds\"] = pd.to_datetime(weather_df[\"ds\"])\n",
    "# Filter the rows between 8 am and 10 pm\n",
    "df_filtered = weather_df[(weather_df['ds'].dt.hour >= 8) & (weather_df['ds'].dt.hour <= 22)]\n",
    "#replace the nan value with 0 in whole dataset\n",
    "df_filtered = df_filtered.fillna(0)\n",
    "\n",
    "#group by he rain by sum\n",
    "df_filtered['sum_rain'] = df_filtered.groupby(pd.Grouper(key='ds', freq='D'))['rain'].transform('sum')\n",
    "df_filtered\n",
    "# Drop all rows where the time component of 'column1' is not 12:00:00\n",
    "df_filtered = weather_df[weather_df['ds'].dt.time == pd.to_datetime('12:00:00').time()]\n",
    "# convert the timestamp column to a datetime object\n",
    "df_filtered['ds'] = pd.to_datetime(df_filtered['ds'])\n",
    "# remove the hour from the ds column\n",
    "df_filtered['ds'] = df_filtered['ds'].dt.normalize()\n",
    "# convert the ds column back to datetime format without hour\n",
    "df_filtered['ds'] = pd.to_datetime(df_filtered['ds']).dt.date\n",
    "df_filtered[\"ds\"] = pd.to_datetime(df_filtered[\"ds\"])\n",
    "#drop the drop_duplicates\n",
    "df_filtered = df_filtered.drop_duplicates()\n",
    "#merge sum weather data after sum rain with df\n",
    "merged_df = pd.merge(df, df_filtered, on='ds', how='left')\n",
    "#drop_duplicates\n",
    "merged_df.drop_duplicates(subset='ds', inplace=True)\n",
    "\n",
    "# holidays BQ query and df\n",
    "query = f\"\"\"\n",
    "SELECT * FROM `{gcp_project}.{bq_dataset}.holidays`\n",
    "\"\"\"\n",
    "df_holiday = client.query(query).to_dataframe()\n",
    "#df_holiday = df_holiday.reset_index()\n",
    "#df_holiday.columns = df_holiday.iloc[0]\n",
    "# drop the first row, which is now redundant\n",
    "# df_holiday = df_holiday.drop(0)\n",
    "df_holiday['ds'] = pd.to_datetime(df_holiday['ds'])\n",
    "df_holiday.info()\n",
    "#merge holiday to df\n",
    "merged_df  = pd.merge(merged_df,df_holiday,on=\"ds\",how=\"left\")\n",
    "#DELETE merged_df = merged_df_h\n",
    "\n",
    "# inflation BQ query and df\n",
    "query = f\"\"\"\n",
    "SELECT * FROM `{gcp_project}.{bq_dataset}.inflation`\n",
    "\"\"\"\n",
    "df_inflation_rate = client.query(query).to_dataframe()\n",
    "df_inflation_rate['ds'] = pd.to_datetime(df_inflation_rate['ds'])\n",
    "#merge inflation to df\n",
    "merged_df= pd.merge(merged_df, df_inflation_rate, how='left', left_on='ds', right_on='ds')\n",
    "\n",
    "# consumption climate query and df\n",
    "query = f\"\"\"\n",
    "SELECT * FROM `{gcp_project}.{bq_dataset}.consumption_climate`\n",
    "\"\"\"\n",
    "df_consumption_climate = client.query(query).to_dataframe()\n",
    "df_consumption_climate['ds'] = pd.to_datetime(df_consumption_climate['ds'])\n",
    "#merge\n",
    "merged_df = pd.merge(merged_df, df_consumption_climate,on=\"ds\",how=\"left\")\n",
    "\n",
    "#covid\n",
    "merged_df['cov_lock'] = ((merged_df['ds'] >= pd.to_datetime(\"2020-03-22\")) & (merged_df['ds'] <= pd.to_datetime(\"2020-04-21\"))) | ((merged_df['ds'] >= pd.to_datetime(\"2020-12-16\")) & (merged_df['ds'] <= pd.to_datetime(\"2021-01-09\"))) | ((merged_df['ds'] >= pd.to_datetime(\"2020-04-22\")) & (merged_df['ds'] <= pd.to_datetime(\"2020-05-03\"))) | ((merged_df['ds'] >= pd.to_datetime(\"2020-10-10\")) & (merged_df['ds'] <= pd.to_datetime(\"2020-12-15\"))) | ((merged_df['ds'] >= pd.to_datetime(\"2021-03-31\")) & (merged_df['ds'] <= pd.to_datetime(\"2021-05-18\"))) | ((merged_df['ds'] >= pd.to_datetime(\"2021-11-02\")) & (merged_df['ds'] <= pd.to_datetime(\"2021-12-09\")))\n",
    "merged_df['cov_lock'] = merged_df['cov_lock'].astype(int)\n",
    "\n",
    "# Berlin Unemployment Mitte and Mitte Mitte query and df\n",
    "query = f\"\"\"\n",
    "SELECT * FROM `{gcp_project}.{bq_dataset}.berlin_unemployment`\n",
    "\"\"\"\n",
    "df_unemp_ber = client.query(query).to_dataframe()\n",
    "# fom object to datetype\n",
    "df_unemp_ber['Date'] = pd.to_datetime(df_unemp_ber['Date'])\n",
    "#df_unemp_ber[['unemp_Berlin_Mitte', 'unemp_Berlin_Mitte_Mitte']] = df_unemp_ber[['unemp_Berlin_Mitte', 'unemp_Berlin_Mitte_Mitte']].apply(lambda x: x.str.replace(',', '.'))\n",
    "#df_unemp_ber = df_unemp_ber.astype({\"unemp_Berlin_Mitte\": float, \"unemp_Berlin_Mitte_Mitte\": float})\n",
    "df_unemp_ber[['unemp_Berlin_Mitte', 'unemp_Berlin_Mitte_Mitte']] = df_unemp_ber[['unemp_Berlin_Mitte', 'unemp_Berlin_Mitte_Mitte']].div(100)\n",
    "df_unemp_ber.set_index('Date', inplace=True)\n",
    "df_unemp_ber = df_unemp_ber.resample('D').ffill()\n",
    "merged_df = pd.merge(merged_df, df_unemp_ber, how='left', left_on='ds', right_on='Date')\n",
    "\n",
    "#Fill NaNs and strip whitespaces\n",
    "merged_df = merged_df.fillna(0)\n",
    "merged_df = merged_df.rename(columns=lambda x: str(x).strip())\n",
    "#Drop unnecessary columns based on correlation or co-correlation\n",
    "merged_df.drop(['visibility', 'dew_point', 'feels_like', 'temp_min', 'temp_max', 'pressure', 'wind_gust',\n",
    "        'snow_1h', 'weather_id', 'weather_main', 'weather_description', 'unemp_Berlin_Mitte_Mitte'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bbe6fb90",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1703 entries, 0 to 1702\n",
      "Data columns (total 13 columns):\n",
      " #   Column               Non-Null Count  Dtype         \n",
      "---  ------               --------------  -----         \n",
      " 0   ds                   1703 non-null   datetime64[ns]\n",
      " 1   y                    1703 non-null   Float64       \n",
      " 2   temp                 1703 non-null   float64       \n",
      " 3   humidity             1703 non-null   Int64         \n",
      " 4   wind_speed           1703 non-null   float64       \n",
      " 5   wind_deg             1703 non-null   Int64         \n",
      " 6   rain                 1703 non-null   float64       \n",
      " 7   clouds               1703 non-null   Int64         \n",
      " 8   Holiday              1703 non-null   Int64         \n",
      " 9   inflation_rate       1703 non-null   float64       \n",
      " 10  consumption_climate  1703 non-null   float64       \n",
      " 11  cov_lock             1703 non-null   int64         \n",
      " 12  unemp_Berlin_Mitte   1703 non-null   float64       \n",
      "dtypes: Float64(1), Int64(4), datetime64[ns](1), float64(6), int64(1)\n",
      "memory usage: 181.4 KB\n"
     ]
    }
   ],
   "source": [
    "merged_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e884c5e5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ds                     datetime64[ns]\n",
       "y                             Float64\n",
       "temp                          float64\n",
       "humidity                        Int64\n",
       "wind_speed                    float64\n",
       "wind_deg                        Int64\n",
       "rain                          float64\n",
       "clouds                          Int64\n",
       "Holiday                         Int64\n",
       "inflation_rate                float64\n",
       "consumption_climate           float64\n",
       "cov_lock                        int64\n",
       "unemp_Berlin_Mitte            float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df.dtypes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
